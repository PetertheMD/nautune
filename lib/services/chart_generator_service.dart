import 'dart:async';
import 'dart:io';
import 'dart:math' show max, min, log;
import 'package:flutter/foundation.dart';
import 'package:flutter/services.dart';
import 'package:fftea/fftea.dart';
import '../models/chart_data.dart';

/// Service for generating rhythm game charts from audio files.
/// Uses SuperFlux-inspired spectral flux onset detection with pitch tracking.
class ChartGeneratorService {
  static ChartGeneratorService? _instance;
  static ChartGeneratorService get instance =>
      _instance ??= ChartGeneratorService._();

  ChartGeneratorService._();

  // FFT parameters - slightly larger window for better frequency resolution
  static const int _windowSize = 2048;
  static const int _hopSize = 441; // ~10ms hop at 44.1kHz for smoother tracking
  static const int _sampleRate = 44100;

  // Onset detection parameters (SuperFlux-inspired)
  static const int _maxFilterSize = 3;  // Moving max filter (frames) - 30ms
  static const int _avgFilterPast = 15;  // Moving average past context (frames) - 150ms
  static const int _avgFilterFuture = 5; // Moving average future context (frames) - 50ms
  static const double _threshold = 1.8;  // Threshold above moving average
  static const int _minOnsetGapMs = 120; // Minimum gap between notes

  // Lane assignment - 5 frequency bands mapped to musical pitch ranges
  // These are optimized for typical pop/rock music frequency distribution
  static const double _subBassMaxFreq = 100;   // Lane 0 - Green (kick drums, sub-bass)
  static const double _bassMaxFreq = 250;      // Lane 1 - Red (bass guitar, low synth)
  static const double _lowMidMaxFreq = 600;    // Lane 2 - Yellow (vocals low, guitar body)
  static const double _highMidMaxFreq = 2000;  // Lane 3 - Blue (vocals, guitar lead)
  // Lane 4 - Orange (treble > 2000Hz - synths, cymbals)

  /// Progress callback for UI updates (0.0 - 1.0)
  ValueNotifier<double> progress = ValueNotifier(0.0);

  /// Generate a chart from an audio file
  Future<ChartData?> generateChart({
    required String audioPath,
    required String trackId,
    required String trackName,
    required String artistName,
    required int durationMs,
  }) async {
    try {
      progress.value = 0.0;

      // Read audio file
      final audioData = await _readAudioFile(audioPath);
      if (audioData == null || audioData.isEmpty) {
        debugPrint('ðŸŽ® ChartGenerator: Failed to read audio file');
        return null;
      }

      progress.value = 0.1;

      // Run onset detection in isolate
      final result = await compute(_processAudioAdvanced, _AudioProcessingParams(
        samples: audioData,
        sampleRate: _sampleRate,
        windowSize: _windowSize,
        hopSize: _hopSize,
        maxFilterSize: _maxFilterSize,
        avgFilterPast: _avgFilterPast,
        avgFilterFuture: _avgFilterFuture,
        threshold: _threshold,
        minOnsetGapMs: _minOnsetGapMs,
        subBassMaxFreq: _subBassMaxFreq,
        bassMaxFreq: _bassMaxFreq,
        lowMidMaxFreq: _lowMidMaxFreq,
        highMidMaxFreq: _highMidMaxFreq,
      ));

      progress.value = 0.9;

      if (result.notes.isEmpty) {
        debugPrint('ðŸŽ® ChartGenerator: No notes detected');
        return null;
      }

      progress.value = 1.0;

      debugPrint('ðŸŽ® ChartGenerator: Generated ${result.notes.length} notes, BPM: ${result.bpm.round()}');

      return ChartData(
        id: '${trackId}_chart',
        trackId: trackId,
        trackName: trackName,
        artistName: artistName,
        notes: result.notes,
        bpm: result.bpm,
        durationMs: durationMs,
        generatedAt: DateTime.now(),
      );
    } catch (e, stack) {
      debugPrint('ðŸŽ® ChartGenerator: Error - $e');
      debugPrint('$stack');
      return null;
    }
  }

  // Method channel for iOS audio decoding
  static const _iosChannel = MethodChannel('com.elysiumdisc.nautune/audio_decoder');

  /// Read raw PCM samples from audio file
  Future<Float64List?> _readAudioFile(String path) async {
    try {
      final file = File(path);
      if (!await file.exists()) {
        debugPrint('ðŸŽ® ChartGenerator: File not found: $path');
        return null;
      }

      if (Platform.isIOS) {
        return _readAudioFileIOS(path);
      } else {
        return _readAudioFileFFmpeg(path);
      }
    } catch (e) {
      debugPrint('ðŸŽ® ChartGenerator: Error reading audio: $e');
      return null;
    }
  }

  /// iOS: Decode audio using native AVFoundation
  Future<Float64List?> _readAudioFileIOS(String path) async {
    try {
      debugPrint('ðŸŽ® ChartGenerator: Decoding audio with AVFoundation (iOS)...');

      final result = await _iosChannel.invokeMethod<Map>('decodeAudio', {
        'path': path,
        'sampleRate': _sampleRate,
      });

      if (result == null) {
        debugPrint('ðŸŽ® ChartGenerator: iOS decoder returned null');
        return null;
      }

      final samples = result['samples'] as Float64List?;
      if (samples == null || samples.isEmpty) {
        debugPrint('ðŸŽ® ChartGenerator: No samples decoded');
        return null;
      }

      debugPrint('ðŸŽ® ChartGenerator: Decoded ${samples.length} samples (${(samples.length / _sampleRate).toStringAsFixed(1)}s)');
      return samples;
    } on PlatformException catch (e) {
      debugPrint('ðŸŽ® ChartGenerator: iOS decode error: ${e.message}');
      return null;
    }
  }

  /// Linux/Desktop: Decode audio using FFmpeg
  Future<Float64List?> _readAudioFileFFmpeg(String path) async {
    debugPrint('ðŸŽ® ChartGenerator: Decoding audio with FFmpeg...');

    final process = await Process.start('ffmpeg', [
      '-i', path,
      '-ac', '1',
      '-ar', '$_sampleRate',
      '-f', 's16le',
      '-acodec', 'pcm_s16le',
      '-v', 'quiet',
      '-',
    ]);

    final completer = Completer<void>();
    final chunks = <List<int>>[];

    process.stdout.listen(
      (chunk) => chunks.add(chunk),
      onDone: () => completer.complete(),
      onError: (e) {
        debugPrint('ðŸŽ® ChartGenerator: FFmpeg stream error: $e');
        completer.complete();
      },
    );

    process.stderr.listen((_) {});

    await completer.future;
    final exitCode = await process.exitCode;

    if (exitCode != 0) {
      debugPrint('ðŸŽ® ChartGenerator: FFmpeg failed with exit code $exitCode');
      return null;
    }

    final totalLength = chunks.fold<int>(0, (sum, chunk) => sum + chunk.length);
    if (totalLength < 2) {
      debugPrint('ðŸŽ® ChartGenerator: No audio data decoded');
      return null;
    }

    final allBytes = Uint8List(totalLength);
    var offset = 0;
    for (final chunk in chunks) {
      allBytes.setAll(offset, chunk);
      offset += chunk.length;
    }

    final byteData = ByteData.sublistView(allBytes);
    final sampleCount = byteData.lengthInBytes ~/ 2;
    final samples = Float64List(sampleCount);

    for (var i = 0; i < sampleCount; i++) {
      final sample = byteData.getInt16(i * 2, Endian.little);
      samples[i] = sample / 32768.0;
    }

    debugPrint('ðŸŽ® ChartGenerator: FFmpeg decoded ${samples.length} samples');
    return samples;
  }
}

/// Parameters for audio processing
class _AudioProcessingParams {
  final Float64List samples;
  final int sampleRate;
  final int windowSize;
  final int hopSize;
  final int maxFilterSize;
  final int avgFilterPast;
  final int avgFilterFuture;
  final double threshold;
  final int minOnsetGapMs;
  final double subBassMaxFreq;
  final double bassMaxFreq;
  final double lowMidMaxFreq;
  final double highMidMaxFreq;

  const _AudioProcessingParams({
    required this.samples,
    required this.sampleRate,
    required this.windowSize,
    required this.hopSize,
    required this.maxFilterSize,
    required this.avgFilterPast,
    required this.avgFilterFuture,
    required this.threshold,
    required this.minOnsetGapMs,
    required this.subBassMaxFreq,
    required this.bassMaxFreq,
    required this.lowMidMaxFreq,
    required this.highMidMaxFreq,
  });
}

/// Result from audio processing
class _ProcessingResult {
  final List<ChartNote> notes;
  final double bpm;

  const _ProcessingResult({required this.notes, required this.bpm});
}

/// Advanced audio processing with SuperFlux-inspired onset detection
/// and pitch-based lane assignment
_ProcessingResult _processAudioAdvanced(_AudioProcessingParams params) {
  final samples = params.samples;
  final windowSize = params.windowSize;
  final hopSize = params.hopSize;
  final sampleRate = params.sampleRate;

  if (samples.length < windowSize * 2) {
    return const _ProcessingResult(notes: [], bpm: 120.0);
  }

  // Create FFT instance
  final fft = FFT(windowSize);

  // Pre-compute Hanning window
  final window = Float64List(windowSize);
  for (int i = 0; i < windowSize; i++) {
    window[i] = 0.5 * (1 - _cos(2 * 3.14159265359 * i / (windowSize - 1)));
  }

  // Compute STFT
  final numFrames = (samples.length - windowSize) ~/ hopSize + 1;
  final binFreq = sampleRate.toDouble() / windowSize;

  // Store log-magnitude spectra for spectral flux
  final logMagSpectra = <Float64List>[];
  // Store centroid for each frame (for pitch tracking)
  final spectroids = Float64List(numFrames);

  // Frequency bin boundaries
  final subBassMaxBin = (params.subBassMaxFreq / binFreq).round();
  final bassMaxBin = (params.bassMaxFreq / binFreq).round();
  final lowMidMaxBin = (params.lowMidMaxFreq / binFreq).round();
  final highMidMaxBin = (params.highMidMaxFreq / binFreq).round();

  for (int frame = 0; frame < numFrames; frame++) {
    final start = frame * hopSize;

    // Apply window
    final windowed = Float64List(windowSize);
    for (int i = 0; i < windowSize; i++) {
      windowed[i] = samples[start + i] * window[i];
    }

    // Compute FFT
    final spectrum = fft.realFft(windowed);
    final magnitudes = spectrum.discardConjugates().magnitudes();

    // Convert to log-magnitude (add small epsilon to avoid log(0))
    final logMags = Float64List(magnitudes.length);
    double sumMag = 0;
    double sumFreqMag = 0;

    for (int i = 0; i < magnitudes.length; i++) {
      final mag = magnitudes[i];
      logMags[i] = log(mag + 1e-10);

      // Compute spectral centroid (weighted average frequency)
      final freq = i * binFreq;
      sumMag += mag;
      sumFreqMag += freq * mag;
    }

    logMagSpectra.add(logMags);

    // Spectral centroid - the "center of mass" of the spectrum
    // This tells us if the sound is predominantly low or high pitched
    spectroids[frame] = sumMag > 0 ? sumFreqMag / sumMag : 500.0;
  }

  // Compute SuperFlux-style spectral flux
  // This uses a maximum filter to track spectral trajectories,
  // making it robust to vibrato and gradual changes
  final spectralFlux = Float64List(numFrames);
  final maxFilter = params.maxFilterSize;

  for (int frame = 1; frame < numFrames; frame++) {
    final curr = logMagSpectra[frame];
    final prev = logMagSpectra[frame - 1];

    double flux = 0;
    for (int bin = 0; bin < curr.length; bin++) {
      // Get maximum of previous frames (trajectory tracking)
      double maxPrev = prev[bin];
      for (int m = 2; m <= maxFilter && frame - m >= 0; m++) {
        final older = logMagSpectra[frame - m][bin];
        if (older > maxPrev) maxPrev = older;
      }

      // Only count positive differences (energy increases)
      final diff = curr[bin] - maxPrev;
      if (diff > 0) {
        flux += diff;
      }
    }
    spectralFlux[frame] = flux;
  }

  // Compute per-band flux for weighting
  final bandFlux = List.generate(5, (_) => Float64List(numFrames));

  for (int frame = 1; frame < numFrames; frame++) {
    final curr = logMagSpectra[frame];
    final prev = logMagSpectra[frame - 1];

    double subBassF = 0, bassF = 0, lowMidF = 0, highMidF = 0, trebleF = 0;

    for (int bin = 0; bin < curr.length; bin++) {
      final diff = curr[bin] - prev[bin];
      if (diff > 0) {
        if (bin < subBassMaxBin) {
          subBassF += diff;
        } else if (bin < bassMaxBin) {
          bassF += diff;
        } else if (bin < lowMidMaxBin) {
          lowMidF += diff;
        } else if (bin < highMidMaxBin) {
          highMidF += diff;
        } else {
          trebleF += diff;
        }
      }
    }

    bandFlux[0][frame] = subBassF;
    bandFlux[1][frame] = bassF;
    bandFlux[2][frame] = lowMidF;
    bandFlux[3][frame] = highMidF;
    bandFlux[4][frame] = trebleF;
  }

  // STEP 1: Estimate BPM first (before onset detection)
  // This allows proper beat-grid quantization
  final bpm = _estimateBpmFromFlux(spectralFlux, hopSize, sampleRate);
  final beatIntervalMs = (60000.0 / bpm).round();
  final sixteenthNoteMs = beatIntervalMs ~/ 4; // 16th note grid

  debugPrint('ðŸŽ® Estimated BPM: ${bpm.round()}, beat interval: ${beatIntervalMs}ms, 16th: ${sixteenthNoteMs}ms');

  // STEP 2: Peak picking with moving average threshold
  final avgPast = params.avgFilterPast;
  final avgFuture = params.avgFilterFuture;
  final threshold = params.threshold;

  // Compute moving average
  final movingAvg = Float64List(numFrames);
  for (int frame = 0; frame < numFrames; frame++) {
    final start = max(0, frame - avgPast);
    final end = min(numFrames, frame + avgFuture + 1);
    double sum = 0;
    for (int i = start; i < end; i++) {
      sum += spectralFlux[i];
    }
    movingAvg[frame] = sum / (end - start);
  }

  // Compute moving maximum
  final movingMax = Float64List(numFrames);
  for (int frame = 0; frame < numFrames; frame++) {
    final start = max(0, frame - maxFilter);
    final end = min(numFrames, frame + maxFilter + 1);
    double maxVal = 0;
    for (int i = start; i < end; i++) {
      if (spectralFlux[i] > maxVal) maxVal = spectralFlux[i];
    }
    movingMax[frame] = maxVal;
  }

  // Detect onsets: peaks that are local maximum AND above threshold
  final onsetFrames = <int>[];
  int lastOnsetFrame = -100;

  for (int frame = 1; frame < numFrames - 1; frame++) {
    // Must be equal to local maximum (within floating point tolerance)
    final isLocalMax = (spectralFlux[frame] - movingMax[frame]).abs() < 1e-6;

    // Must exceed adaptive threshold
    final aboveThreshold = spectralFlux[frame] > movingAvg[frame] * threshold;

    if (isLocalMax && aboveThreshold) {
      // Enforce minimum gap
      final timestampMs = (frame * hopSize * 1000 / sampleRate).round();
      final lastTimestampMs = lastOnsetFrame >= 0
          ? (lastOnsetFrame * hopSize * 1000 / sampleRate).round()
          : -1000;

      if (timestampMs - lastTimestampMs >= params.minOnsetGapMs) {
        onsetFrames.add(frame);
        lastOnsetFrame = frame;
      }
    }
  }

  debugPrint('ðŸŽ® Detected ${onsetFrames.length} raw onsets');

  // STEP 3: Create notes with beat-quantized timing and pitch-based lanes
  final notes = <ChartNote>[];

  for (final frame in onsetFrames) {
    final rawTimestampMs = (frame * hopSize * 1000 / sampleRate).round();

    // Quantize to 16th note grid
    final quantizedMs = ((rawTimestampMs + sixteenthNoteMs ~/ 2) ~/ sixteenthNoteMs) * sixteenthNoteMs;

    // Determine lane based on BOTH spectral centroid (pitch) and band flux
    // The centroid tells us the "pitch feel" of this moment in the song
    final centroid = spectroids[frame];

    // Map centroid to a rough lane (0-4 based on frequency range)
    int pitchLane;
    if (centroid < 150) {
      pitchLane = 0; // Very low - sub-bass
    } else if (centroid < 350) {
      pitchLane = 1; // Low - bass
    } else if (centroid < 800) {
      pitchLane = 2; // Mid-low - vocals/guitar
    } else if (centroid < 1500) {
      pitchLane = 3; // Mid-high - lead vocals/synth
    } else {
      pitchLane = 4; // High - treble
    }

    // Also check which band had the strongest onset
    // Weight: bass gets boost, treble gets reduced
    final weights = [3.0, 2.5, 2.0, 1.5, 0.5]; // sub-bass, bass, low-mid, high-mid, treble
    double maxWeightedFlux = 0;
    int fluxLane = 2; // default to middle

    for (int i = 0; i < 5; i++) {
      final weighted = bandFlux[i][frame] * weights[i];
      if (weighted > maxWeightedFlux) {
        maxWeightedFlux = weighted;
        fluxLane = i;
      }
    }

    // Combine: prefer flux-based lane for drums/bass, pitch-based for melodic content
    // If sub-bass or bass flux is strong, use flux lane (it's probably a kick/bass hit)
    // Otherwise, use pitch lane (it's probably melodic content)
    final bassFluxRatio = (bandFlux[0][frame] + bandFlux[1][frame]) /
        (spectralFlux[frame] + 1e-6);

    final lane = bassFluxRatio > 0.4 ? fluxLane : pitchLane;

    notes.add(ChartNote(
      timestampMs: quantizedMs,
      lane: lane,
      band: FrequencyBand.values[lane],
    ));

    // Cap at 3000 notes for very long tracks
    if (notes.length >= 3000) break;
  }

  // Remove duplicate timestamps (can happen after quantization)
  final uniqueNotes = <ChartNote>[];
  final seenTimestamps = <int>{};
  for (final note in notes) {
    if (!seenTimestamps.contains(note.timestampMs)) {
      seenTimestamps.add(note.timestampMs);
      uniqueNotes.add(note);
    }
  }

  // Debug: Lane distribution
  final laneCounts = [0, 0, 0, 0, 0];
  for (final note in uniqueNotes) {
    laneCounts[note.lane]++;
  }
  debugPrint('ðŸŽ® Final: ${uniqueNotes.length} notes, lanes: $laneCounts');

  return _ProcessingResult(notes: uniqueNotes, bpm: bpm);
}

/// Estimate BPM from spectral flux using autocorrelation
double _estimateBpmFromFlux(Float64List flux, int hopSize, int sampleRate) {
  // Look for periodicities in flux corresponding to 60-200 BPM
  // At 44100Hz and 441 hop, each frame is ~10ms
  // 60 BPM = 1000ms per beat = 100 frames
  // 200 BPM = 300ms per beat = 30 frames

  final minLag = 30;  // 200 BPM
  final maxLag = 100; // 60 BPM

  // Normalize flux
  double mean = 0;
  for (int i = 0; i < flux.length; i++) {
    mean += flux[i];
  }
  mean /= flux.length;

  final normalizedFlux = Float64List(flux.length);
  for (int i = 0; i < flux.length; i++) {
    normalizedFlux[i] = flux[i] - mean;
  }

  // Compute autocorrelation for different lags
  double bestCorr = 0;
  int bestLag = 50; // default to ~120 BPM

  for (int lag = minLag; lag <= maxLag; lag++) {
    double corr = 0;
    int count = 0;

    for (int i = 0; i < normalizedFlux.length - lag; i++) {
      corr += normalizedFlux[i] * normalizedFlux[i + lag];
      count++;
    }

    if (count > 0) {
      corr /= count;
      if (corr > bestCorr) {
        bestCorr = corr;
        bestLag = lag;
      }
    }
  }

  // Convert lag to BPM
  final lagMs = bestLag * hopSize * 1000.0 / sampleRate;
  final bpm = 60000.0 / lagMs;

  // Clamp to reasonable range
  return bpm.clamp(70.0, 180.0);
}

double _cos(double x) {
  x = x % (2 * 3.14159265359);
  double result = 1.0;
  double term = 1.0;
  for (int i = 1; i <= 10; i++) {
    term *= -x * x / ((2 * i - 1) * (2 * i));
    result += term;
  }
  return result;
}
